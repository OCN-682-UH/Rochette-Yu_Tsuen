---
author: 'Keanu Rochette '
date: '`r format(Sys.Date())`'
title: Week 8 - Strings (1/2)
output: html_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE)
```


## Load the Libraries

```{r}
library(tidyverse)
library(here)
library(tidytext)
library(wordcloud2)
library(janeaustenr)
library(stopwords)
```

## Definition
String : basically a character, in quotes.

## Stringr intro:
### 1. Manipulation 
example : can be useful for plot names 
```{r}
paste("string 1", "string 2")

paste("string 1", "string 2", sep = "-") # puts spec separation in betwe

paste0("string 1", "string 2") #no space between the strings
```

```{r}
shapes <- c("square", "triangle", "circle")
paste("my favorite shape is", shapes)
```
```{r}
str_length(shapes) #how many letters in each word
```
```{r}
seq_data<- c("ATCCCGTC")

str_sub(seq_data, start = 2, end = 4)
```

```{r}
str_sub(seq_data, start = 3, end = 3) <- "A" # change 3 charact to A
seq_data

```

```{r}
str_dup(seq_data, times = c(2, 3)) # "times" duplicate string X amount
# 2 = 2x, 3 = 3x duplicate
```

### 2. Whitespace tools  

Remove the whitespaces: str_trim
```{r}
badtreatments <- c("High", " High", "High ", "Low", "Low")

str_trim(badtreatments) #removes all white spaces 

str_trim(badtreatments, side = "left") #removes white spaces on leftside only
```

Add whitespaces: str_pad
```{r}
str_pad(badtreatments, 5, side = "right") # add a white space to the right side after the 5th character
```
Add a character instead of white space
```{r}
str_pad(badtreatments, 5, side = "right", pad = "1") 
# add a 1 to the right side so that all the strings are 5 charct long
```

### 3. Locale sensitive operations: 
Important, these will perform differently in different places in the world/with different languages. The default is English, but you can set the language setting.

Make everything upper case:
```{r}
x<-"I love R!"
str_to_upper(x)
```
Make it lower case:
```{r}
str_to_lower(x)
```

Make it title case (Cap first letter of each word)
```{r}
str_to_title(x)
```
4. Pattern matching function:  

{stringr} has functions to view, detect, locate, extract, match, replace, and split strings based on specific patterns.

View a specific pattern in a vector of strings.
```{r}
data<-c("AAA", "TATA", "CTAG", "GCTT")

# find all the strings with an A
str_view(data, pattern = "A")
```
Detect a specific pattern:
```{r}
str_detect(data, pattern = "A")

str_detect(data, pattern = "AT")

```

Locate a specific pattern:
```{r}
str_locate(data, pattern = "AT") #gives position of the AT
```

## regex: Regular expressions
- Metacharacters
- Sequences
- Quantifiers
- Character classes
- POSIX character classes (Portable Operating System Interface)

### Metacharacters
Special characters that have reserved meaning: . \ | ( ) [ { $ * + ?


Example: finding periods, replacing periods...
```{r}
vals<-c("a.b", "b.c","c.d")
```

Replacing "." by space
```{r}
#string, pattern, replace
str_replace(vals, "\\.", " ") # need \\ to escape the metacharacter meaning 
```
Let's say we had multiple "." in our character vector
```{r}
vals<-c("a.b.c", "b.c.d","c.d.e")
#string, pattern, replace
str_replace(vals, "\\.", " ")
 
#only replaced one of the "."
```
To replace all, str_replace_all()
```{r}
#string, pattern, replace
str_replace_all(vals, "\\.", " ")
```


### Sequences:
letters have special meanings too ! 
Need a list of the hidden meaning to better use them.

Example : Let's subset the vector to only keep strings with digits
```{r}
val2<-c("test 123", "test 456", "test")
str_subset(val2, "\\d")

#finds all the strings that have numbers in them 
# \\d : digits
```

### Character class
list of characters enclosed by square brackets [ ]
```{r}
str_count(val2, "[aeiou]") # "e" only is counted in each string = 1
```
```{r}
# count any digit
str_count(val2, "[0-9]")
#counts the number of digits in each string
```

### Quantifiers
More special characters. Need practice. 
```{r}
strings<-c("550-153-7578",
         "banana",
         "435.114.7586",
         "home: 672-442-6739")
```

```{r}
phone <- "([2-9][0-9]{2})[- .]([0-9]{3})[- .]([0-9]{4})"

```
Translation : 
> ([num btwn 2-9][num 0-9]{apply 2x on the last search box})
> [- .] separated by - or .
> ([num 0-9]{apply 3x on the last search box}) 
> [- .] separated by - or .
> ([num 0-9]{apply 4x on the last search box})

```{r}
# Which strings contain phone numbers?
str_detect(strings, phone)
```

```{r}
# subset only the strings with phone numbers
test<-str_subset(strings, phone)
test
```


#### Practice 
```{r}
strings %>%  
  str_subset(phone) %>% 
  str_replace("[a-z]{4}", "") %>% 
  str_replace(":","") %>% str_trim() %>% 
  str_replace_all("\\.", "-")
  
```
Answer: 
```{r}
test %>%
    # replace "." with "-"
  str_replace_all(pattern = "\\.", replacement = "-") %>% 
    # remove all the things we don't want, find all the letters OR ":"
    # replace with nothing
  str_replace_all(pattern = "[a-zA-Z]|\\:", replacement = "") %>% 
  # trim the white space
  str_trim() 


```


## tidytext
Play with the books of Jane Austen

```{r}
head(austen_books())
```


Let's clean it up and add a column for line and chapter

```{r}
original_books <- austen_books() %>% # get all of Jane Austen's books
  group_by(book) %>%
  mutate(line = row_number(), # find every line
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", 
        # detect in the text the word "chapter" and digits (\\d) or roman num
        # cumsum: cumulative sum
                                                 ignore_case = TRUE)))) %>%
                                                #ignore lower or uppercase
  ungroup() # ungroup it so we have a dataframe again
# don't try to view the entire thing... its >73000 lines...
head(original_books)
```

Look for words per line
```{r}
tidy_books <- original_books %>%
  unnest_tokens(output = word, input = text) # add a column named word, with the input as the text column
head(tidy_books) # there are now >725,000 rows. Don't view the entire thing!
```

### Removing stopwords 
```{r}
head(get_stopwords()) 
#those are stopwords according to specific lexicons
```

```{r}
#antijoining so we only keep the words that don't belong in the stop words list
cleaned_books <- tidy_books %>%
  anti_join(get_stopwords()) # dataframe without the stopwords

head(cleaned_books)
```


Most used words in the books
```{r}
cleaned_books %>%
  count(word, sort = TRUE)

```

### Sentiment Analysis 
```{r}
sent_word_counts <- tidy_books %>%
  inner_join(get_sentiments()) %>% # only keep pos or negative words
  count(word, sentiment, sort = TRUE) # count them
head(sent_word_counts)[1:3,]
```

## Plotting the data
We can now use ggplot to visualize counts of positive and negative words in the books
```{r}
sent_word_counts %>%
  filter(n > 150) %>% # take only if there are over 150 instances of it
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>% # add a column where if the word is negative make the count negative
  mutate(word = reorder(word, n)) %>% # sort it so it goes from largest to smallest
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col() +
  coord_flip() +
  labs(y = "Contribution to sentiment")
```

### Word Cloud

```{r}
words<-cleaned_books %>%
  count(word) %>% # count all the words
  arrange(desc(n))%>% # sort the words
  slice(1:100) #take the top 100
wordcloud2(words, shape = 'triangle', size=0.3) # make a wordcloud out of the top 100 words
```





